import time
import random
from typing import Dict, List, Any

class WorkflowEngine:
    """
    ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³
    è¤‡æ•°ã®AIã‚¿ã‚¹ã‚¯ã‚’é †ç•ªã«å®Ÿè¡Œã—ã€å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœã‚’æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«å¼•ãç¶™ãã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(self, builder_class, registry, goal_index, conflict_map, schema_registry):
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œã‚‹å·¥å ´ï¼ˆBuilderï¼‰ã®è¨­å®šã‚’ä¿å­˜
        self.builder_config = {
            "registry": registry,
            "goal_index": goal_index,
            "conflict_map": conflict_map,
            "schema_registry": schema_registry
        }
        self.builder_class = builder_class
        self.steps = []  # æ‰‹é †ãƒªã‚¹ãƒˆ
        self.memory = []  # ä¼šè©±ã®å±¥æ­´ï¼ˆè¨˜æ†¶ï¼‰

    def add_step(self, step_name: str, goal: str, schema_id: str = None):
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«æ–°ã—ã„æ‰‹é †ï¼ˆã‚¹ãƒ†ãƒƒãƒ—ï¼‰ã‚’è¿½åŠ ã™ã‚‹"""
        print(f"â• ã‚¹ãƒ†ãƒƒãƒ—è¿½åŠ : [{step_name}] (ç›®çš„: {goal})")
        
        # ã“ã®ã‚¹ãƒ†ãƒƒãƒ—å°‚ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ“ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
        builder = self.builder_class(**self.builder_config)
        
        # ç›®çš„ã«å¿œã˜ã¦è‡ªå‹•ã§éƒ¨å“ã‚’é¸ã¶ï¼ˆã“ã‚Œã¾ã§ã®æ©Ÿèƒ½ã‚’å†åˆ©ç”¨ï¼ï¼‰
        builder.recommend_by_goal(goal)
        
        # å‡ºåŠ›å½¢å¼ãŒã‚ã‚Œã°ã‚»ãƒƒãƒˆ
        if schema_id:
            builder.set_output_schema(schema_id)
            
        self.steps.append({
            "name": step_name,
            "builder": builder,
            "goal": goal,
            "schema_id": schema_id
        })

    def list_steps(self):
        """ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã®ä¸€è¦§ã‚’è¡¨ç¤º"""
        if not self.steps:
            print("ğŸ“‹ ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã‚¹ãƒ†ãƒƒãƒ—ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        print("ã€ğŸ“ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¹ãƒ†ãƒƒãƒ—ä¸€è¦§ã€‘")
        print("-" * 60)
        for i, step in enumerate(self.steps, 1):
            schema_info = f" â†’ ã‚¹ã‚­ãƒ¼ãƒ: {step['schema_id']}" if step['schema_id'] else ""
            print(f"{i}. {step['name']} (ç›®çš„: {step['goal']}){schema_info}")
        print("-" * 60)

    def remove_step(self, step_index: int):
        """ã‚¹ãƒ†ãƒƒãƒ—ã‚’å‰Šé™¤ã™ã‚‹ï¼ˆ0å§‹ã¾ã‚Šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰"""
        if 0 <= step_index < len(self.steps):
            removed = self.steps.pop(step_index)
            print(f"ğŸ—‘ï¸ ã‚¹ãƒ†ãƒƒãƒ—ã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {removed['name']}")
        else:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {step_index} ã¯ç¯„å›²å¤–ã§ã™ã€‚")

    def clear_steps(self):
        """ã™ã¹ã¦ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚¯ãƒªã‚¢"""
        count = len(self.steps)
        self.steps = []
        print(f"ğŸ§¹ {count}å€‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")

    def run(self, initial_input: str, simulate: bool = True):
        """
        ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œã™ã‚‹
        
        Args:
            initial_input: æœ€åˆã®å…¥åŠ›
            simulate: Trueã®å ´åˆã¯AIå¿œç­”ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã€Falseã®å ´åˆã¯å®Ÿéš›ã®APIå‘¼ã³å‡ºã—
        """
        print(f"\nğŸš€ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’é–‹å§‹ã—ã¾ã™ã€‚å…¥åŠ›: ã€Œ{initial_input}ã€\n")
        
        current_context = initial_input
        
        for i, step in enumerate(self.steps):
            step_name = step["name"]
            builder = step["builder"]
            
            print(f"\n--- Step {i+1}: {step_name} å®Ÿè¡Œä¸­... ---")
            
            # 1. å±¥æ­´ã‚’å«ã‚ãŸå¤‰æ•°ã‚’æº–å‚™
            # å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã®æˆæœç‰©ã‚’ã€ŒèƒŒæ™¯ã€ã¨ã—ã¦æ¸¡ã™ã“ã¨ã§ã€æ–‡è„ˆã‚’ã¤ãªã
            variables = {
                "max_tokens": "1000",
                "context": f"ã“ã‚Œã¾ã§ã®çµŒç·¯:\n{self._format_memory()}",
                "objective": f"ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯: {current_context} ã«åŸºã¥ã„ã¦å‡¦ç†ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚",
                "style": "è«–ç†çš„ã‹ã¤å‰µé€ çš„ã«ã€‚",
                "tone": "ä¸å¯§ãªå£èª¿ã€‚",
                "audience": "æ¬¡ã®å·¥ç¨‹ã®æ‹…å½“è€…ã€‚",
                "response_format": "æŒ‡å®šã•ã‚ŒãŸå½¢å¼ã€‚"
            }
            
            # 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆï¼ˆã‚¹ã‚­ãƒ¼ãƒä»˜ãï¼‰
            if step["schema_id"]:
                prompt = builder.build_with_schema(variables)
            else:
                prompt = builder.build(variables)
            
            # 3. AIã®å®Ÿè¡Œ
            if simulate:
                ai_response = self._simulate_ai_response(step_name)
            else:
                # å®Ÿéš›ã®APIå‘¼ã³å‡ºã—ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå®Ÿè£…ã™ã‚‹ï¼‰
                ai_response = self._call_ai_api(prompt, step_name)
            
            # 4. çµæœã‚’è¨˜æ†¶ã«ä¿å­˜
            self.memory.append(f"ã€{step_name}ã®æˆæœç‰©ã€‘: {ai_response}")
            current_context = ai_response  # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸ã®å…¥åŠ›ã«ãªã‚‹
            
            print(f"âœ… {step_name} å®Œäº†ã€‚")
            time.sleep(0.5)  # å‡¦ç†ã—ã¦ã‚‹æ„Ÿã‚’å‡ºã™æ¼”å‡º

        print("\nğŸ‰ å…¨ã‚¹ãƒ†ãƒƒãƒ—å®Œäº†ï¼")
        return self.memory

    def get_memory(self) -> List[str]:
        """ãƒ¡ãƒ¢ãƒªï¼ˆå®Ÿè¡Œå±¥æ­´ï¼‰ã‚’å–å¾—"""
        return self.memory.copy()

    def clear_memory(self):
        """ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢"""
        count = len(self.memory)
        self.memory = []
        print(f"ğŸ§¹ {count}å€‹ã®ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")

    def show_memory(self):
        """ãƒ¡ãƒ¢ãƒªã®å†…å®¹ã‚’è¡¨ç¤º"""
        if not self.memory:
            print("ğŸ“‹ ãƒ¡ãƒ¢ãƒªã¯ç©ºã§ã™ã€‚")
            return
        
        print("ã€ğŸ’­ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ¡ãƒ¢ãƒªã€‘")
        print("-" * 60)
        for i, mem in enumerate(self.memory, 1):
            print(f"{i}. {mem}")
        print("-" * 60)

    def _format_memory(self) -> str:
        """è¨˜æ†¶ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–ã™ã‚‹"""
        if not self.memory:
            return "ï¼ˆã¾ã å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰"
        return "\n".join(self.memory)

    def _simulate_ai_response(self, step_name: str) -> str:
        """AIã®è¿”ç­”ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹é–¢æ•°ï¼ˆå®Ÿéš›ã¯APIã‚’å‘¼ã¶å ´æ‰€ï¼‰"""
        return f"ï¼ˆã“ã“ã§AIãŒã€Œ{step_name}ã€ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã€ç´ æ™´ã‚‰ã—ã„çµæœã‚’å‡ºåŠ›ã—ã¾ã—ãŸ...ï¼‰"

    def _call_ai_api(self, prompt: str, step_name: str) -> str:
        """
        å®Ÿéš›ã®AI APIã‚’å‘¼ã³å‡ºã™é–¢æ•°ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå®Ÿè£…ã™ã‚‹ï¼‰
        
        ä¾‹:
        import openai
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
        """
        raise NotImplementedError(
            "å®Ÿéš›ã®AI APIå‘¼ã³å‡ºã—ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ã“ã®é–¢æ•°ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚\n"
            "ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€run(simulate=True)ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
        )

    def export_workflow(self, filepath: str):
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®šç¾©ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        import json
        
        workflow_data = {
            "steps": [
                {
                    "name": step["name"],
                    "goal": step["goal"],
                    "schema_id": step["schema_id"]
                }
                for step in self.steps
            ]
        }
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(workflow_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def import_workflow(self, filepath: str):
        """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®šç¾©ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        import json
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                workflow_data = json.load(f)
            
            # æ—¢å­˜ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚¯ãƒªã‚¢
            self.steps = []
            
            # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ãŸã‚¹ãƒ†ãƒƒãƒ—ã‚’è¿½åŠ 
            for step_data in workflow_data.get('steps', []):
                self.add_step(
                    step_data['name'],
                    step_data['goal'],
                    step_data.get('schema_id')
                )
            
            print(f"ğŸ“‚ {len(self.steps)}å€‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {filepath}")
        except FileNotFoundError:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
        except json.JSONDecodeError:
            print(f"âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ãŒä¸æ­£ã§ã™: {filepath}")
        except Exception as e:
            print(f"âŒ èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

# ============================================================
# ğŸ” EVALUATOR (è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ )
# ============================================================

class Evaluator:
    """
    AIã®å‡ºåŠ›ã‚’è©•ä¾¡ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ 
    åˆæ ¼/ä¸åˆæ ¼ã‚’åˆ¤å®šã—ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›
    """
    
    def __init__(self):
        pass

    def evaluate(self, ai_output: str, criteria: List[str]) -> Dict:
        """
        AIã®å‡ºåŠ›ã‚’è©•ä¾¡ã™ã‚‹ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        å®Ÿéš›ã«ã¯ã€ã“ã“ã§åˆ¥ã®LLMã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æŠ•ã’ã¦æ¡ç‚¹ã•ã›ã¾ã™ã€‚
        
        Args:
            ai_output: AIã®å‡ºåŠ›
            criteria: è©•ä¾¡åŸºæº–ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            è©•ä¾¡çµæœã®è¾æ›¸ (is_passed, score, feedback)
        """
        print("\nğŸ” --- å¯©æŸ»å“¡(Evaluator)ãŒãƒã‚§ãƒƒã‚¯ä¸­... ---")
        
        # ä»Šå›ã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã—ã¦ã€ãƒ©ãƒ³ãƒ€ãƒ ã«ç‚¹æ•°ã¨ã‚³ãƒ¡ãƒ³ãƒˆã‚’è¿”ã—ã¾ã™
        # â€»å®Ÿéš›ã¯ã€ai_outputã®ä¸­èº«ã‚’è§£æã—ã¾ã™
        
        score = random.randint(40, 100)  # 40~100ç‚¹ã§ãƒ©ãƒ³ãƒ€ãƒ æ¡ç‚¹
        
        if score >= 80:
            result = {
                "is_passed": True,
                "score": score,
                "feedback": "ç´ æ™´ã‚‰ã—ã„ï¼è¦ä»¶ã‚’å®Œå…¨ã«æº€ãŸã—ã¦ã„ã¾ã™ã€‚"
            }
        else:
            # ç‚¹æ•°ãŒä½ã„å ´åˆã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ä¾‹
            feedbacks = [
                "JSONã®å½¢å¼ãŒå´©ã‚Œã¦ã„ã¾ã™ã€‚",
                "å…·ä½“ä¾‹ãŒä¸è¶³ã—ã¦ãŠã‚Šã€æŠ½è±¡çš„ã™ãã¾ã™ã€‚",
                "ãƒˆãƒ¼ãƒ³ãŒæŒ‡ç¤ºï¼ˆä¸å¯§èªï¼‰ã¨ç•°ãªã‚Šã€ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ã™ãã¾ã™ã€‚",
                "æŒ‡å®šã•ã‚ŒãŸæ–‡å­—æ•°ã‚’è¶…éã—ã¦ã„ã¾ã™ã€‚"
            ]
            result = {
                "is_passed": False,
                "score": score,
                "feedback": random.choice(feedbacks)  # ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒ€ãƒ¡å‡ºã—
            }
            
        print(f"   ğŸ“Š ã‚¹ã‚³ã‚¢: {result['score']}ç‚¹")
        print(f"   ğŸ’¬ åˆ¤å®š: {'åˆæ ¼ âœ…' if result['is_passed'] else 'ä¸åˆæ ¼ âŒ'}")
        print(f"   ğŸ“ ã‚³ãƒ¡ãƒ³ãƒˆ: {result['feedback']}")
        
        return result

    def evaluate_with_llm(self, ai_output: str, criteria: List[str]) -> Dict:
        """
        å®Ÿéš›ã®LLMã‚’ä½¿ç”¨ã—ã¦è©•ä¾¡ã™ã‚‹ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå®Ÿè£…ã™ã‚‹ï¼‰
        
        ä¾‹:
        import openai
        evaluation_prompt = f'''
        ä»¥ä¸‹ã®å‡ºåŠ›ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
        
        è©•ä¾¡åŸºæº–:
        {chr(10).join(f"- {c}" for c in criteria)}
        
        å‡ºåŠ›:
        {ai_output}
        
        0-100ç‚¹ã§ã‚¹ã‚³ã‚¢ã‚’ä»˜ã‘ã€åˆæ ¼/ä¸åˆæ ¼ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
        '''
        
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": evaluation_prompt}]
        )
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦è¿”ã™
        """
        raise NotImplementedError(
            "å®Ÿéš›ã®LLMè©•ä¾¡ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ã“ã®é–¢æ•°ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚\n"
            "ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€evaluate()ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
        )


# WorkflowEngineã‚¯ãƒ©ã‚¹ã«è©•ä¾¡æ©Ÿèƒ½ã‚’è¿½åŠ ã™ã‚‹ãŸã‚ã®æ‹¡å¼µãƒ¡ã‚½ãƒƒãƒ‰
# ä»¥ä¸‹ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’WorkflowEngineã‚¯ãƒ©ã‚¹ã«è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™

def _add_evaluation_to_workflow_engine():
    """
    WorkflowEngineã‚¯ãƒ©ã‚¹ã« run_with_evaluation ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‹•çš„ã«è¿½åŠ 
    """
    def run_with_evaluation(
        self, 
        initial_input: str, 
        criteria: List[str], 
        max_retries: int = 3,
        simulate: bool = True
    ):
        """
        è©•ä¾¡ä»˜ãã§ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œã™ã‚‹
        ä¸åˆæ ¼ã®å ´åˆã¯è‡ªå‹•çš„ã«ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹
        
        Args:
            initial_input: æœ€åˆã®å…¥åŠ›
            criteria: è©•ä¾¡åŸºæº–ã®ãƒªã‚¹ãƒˆ
            max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
            simulate: ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰
            
        Returns:
            æœ€çµ‚çš„ãªãƒ¡ãƒ¢ãƒªã¨è©•ä¾¡çµæœ
        """
        print(f"\nğŸš€ è©•ä¾¡ä»˜ããƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
        print(f"   æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°: {max_retries}å›")
        print(f"   è©•ä¾¡åŸºæº–: {', '.join(criteria)}\n")
        
        evaluator = Evaluator()
        attempt = 0
        
        while attempt < max_retries:
            attempt += 1
            print(f"\n{'='*60}")
            print(f"ã€è©¦è¡Œ {attempt}/{max_retries}ã€‘")
            print(f"{'='*60}")
            
            # ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ï¼ˆå‰å›ã®å¤±æ•—ã‚’ãƒªã‚»ãƒƒãƒˆï¼‰
            if attempt > 1:
                self.clear_memory()
            
            # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
            results = self.run(initial_input, simulate=simulate)
            
            # æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã®å‡ºåŠ›ã‚’è©•ä¾¡
            final_output = results[-1] if results else ""
            
            # è©•ä¾¡ã‚’å®Ÿè¡Œ
            eval_result = evaluator.evaluate(final_output, criteria)
            
            if eval_result['is_passed']:
                print(f"\nğŸŠ æˆåŠŸï¼{attempt}å›ç›®ã®è©¦è¡Œã§åˆæ ¼ã—ã¾ã—ãŸã€‚")
                return {
                    "memory": results,
                    "evaluation": eval_result,
                    "attempts": attempt
                }
            else:
                print(f"\nğŸ˜ ä¸åˆæ ¼... ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯: {eval_result['feedback']}")
                if attempt < max_retries:
                    print(f"   ğŸ”„ ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...ï¼ˆæ®‹ã‚Š{max_retries - attempt}å›ï¼‰")
                    # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æ¬¡ã®è©¦è¡Œã®å…¥åŠ›ã«è¿½åŠ 
                    initial_input = f"{initial_input}\n\nã€å‰å›ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã€‘: {eval_result['feedback']}"
                else:
                    print(f"\nâŒ æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ã¾ã—ãŸã€‚")
        
        # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ãŸå ´åˆ
        return {
            "memory": results,
            "evaluation": eval_result,
            "attempts": attempt,
            "status": "failed"
        }
    
    # WorkflowEngineã‚¯ãƒ©ã‚¹ã«ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 
    WorkflowEngine.run_with_evaluation = run_with_evaluation

# è©•ä¾¡æ©Ÿèƒ½ã‚’è¿½åŠ 
_add_evaluation_to_workflow_engine()

# ============================================================
# ğŸ§  INTENT ANALYZER (æ„å›³åˆ†æå™¨)
# ============================================================

class IntentAnalyzer:
    """
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã—ã€é©åˆ‡ãªè©•ä¾¡åŸºæº–ã‚’è‡ªå‹•é¸æŠã™ã‚‹
    ãƒ¡ã‚¿èªçŸ¥ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚‹é©å¿œçš„è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(self, criteria_lib: Dict):
        self.criteria_lib = criteria_lib

    def analyze_intent(self, prompt_text: str) -> List[str]:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å†…å®¹ã‹ã‚‰ã€Œç‹™ã„ã€ã‚’èª­ã¿å–ã‚Šã€é©åˆ‡ãªè©•ä¾¡åŸºæº–IDã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        
        Args:
            prompt_text: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            è©•ä¾¡åŸºæº–IDã®ãƒªã‚¹ãƒˆ
        """
        print("ğŸ§  --- ãƒ¡ã‚¿èªçŸ¥ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ„å›³ã‚’è§£æä¸­... ---")
        
        selected_criteria_ids = []
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ã€æ„å›³ã‚’æ±²ã¿å–ã‚‹
        # ï¼ˆå®Ÿéš›ã«ã¯ã“ã“ã§LLMã«ã€Œã“ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è©•ä¾¡è»¸ã‚’3ã¤é¸ã‚“ã§ã€ã¨èãã®ãŒãƒ™ã‚¹ãƒˆã§ã™ï¼‰
        for c_id, c_data in self.criteria_lib.items():
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸­ã«ã€ãã®åŸºæº–ã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ
            for keyword in c_data['keywords']:
                if keyword in prompt_text:
                    selected_criteria_ids.append(c_id)
                    break  # ãƒ’ãƒƒãƒˆã—ãŸã‚‰ãã®åŸºæº–ã¯æ¡ç”¨
        
        # ã‚‚ã—ä½•ã‚‚ãƒ’ãƒƒãƒˆã—ãªã‘ã‚Œã°ã€æœ€ä½é™ã€Œæ­£ç¢ºæ€§ã€ã‚’å…¥ã‚Œã‚‹
        if not selected_criteria_ids:
            selected_criteria_ids = ["accuracy"]
        
        # é‡è¤‡ã‚’å‰Šé™¤
        selected_criteria_ids = list(set(selected_criteria_ids))
        
        print(f"   ğŸ“‹ é¸æŠã•ã‚ŒãŸè©•ä¾¡åŸºæº–: {[self.criteria_lib[cid]['name'] for cid in selected_criteria_ids]}")
        
        return selected_criteria_ids

    def list_available_criteria(self):
        """åˆ©ç”¨å¯èƒ½ãªè©•ä¾¡åŸºæº–ã®ä¸€è¦§ã‚’è¡¨ç¤º"""
        print("ã€ğŸ¯ åˆ©ç”¨å¯èƒ½ãªè©•ä¾¡åŸºæº–ã€‘")
        print("-" * 60)
        for c_id, c_data in self.criteria_lib.items():
            print(f"ğŸ‘‰ {c_id.ljust(20)} : {c_data['name']}")
            print(f"   {c_data['description']}")
            print(f"   ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(c_data['keywords'])}")
            print()
        print("-" * 60)


# ============================================================
# ğŸ¯ ADAPTIVE EVALUATOR (é©å¿œçš„è©•ä¾¡å™¨)
# ============================================================

class AdaptiveEvaluator:
    """
    IntentAnalyzerã‚’ä½¿ç”¨ã—ã¦ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ„å›³ã«åŸºã¥ã„ãŸé©å¿œçš„è©•ä¾¡ã‚’è¡Œã†
    ãƒ¡ã‚¿èªçŸ¥è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®ä¸­æ ¸
    """
    
    def __init__(self, analyzer: IntentAnalyzer):
        self.analyzer = analyzer

    def evaluate_prompt_effectiveness(self, prompt_text: str, ai_output_simulation: str) -> Dict:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¦‹ã¦è©•ä¾¡åŸºæº–ã‚’æ±ºã‚ã€ãã®åŸºæº–ã§å‡ºåŠ›ã‚’æ¡ç‚¹ã™ã‚‹
        
        Args:
            prompt_text: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆ
            ai_output_simulation: AIã®å‡ºåŠ›ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            è©•ä¾¡çµæœã®è¾æ›¸
        """
        # 1. ã¾ãšãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ„å›³ã‚’ç†è§£ã™ã‚‹ï¼ˆãƒ¡ã‚¿èªçŸ¥ï¼‰
        active_criteria_ids = self.analyzer.analyze_intent(prompt_text)
        
        print(f"\nğŸ¯ æŠ½å‡ºã•ã‚ŒãŸè©•ä¾¡è¦³ç‚¹: {[self.analyzer.criteria_lib[cid]['name'] for cid in active_criteria_ids]}")
        
        # 2. é¸ã°ã‚ŒãŸåŸºæº–ã”ã¨ã«æ¡ç‚¹ã™ã‚‹ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        print("\nğŸ“ --- æ¡ç‚¹é–‹å§‹ ---")
        total_score = 0
        report = {}
        
        for cid in active_criteria_ids:
            criteria_name = self.analyzer.criteria_lib[cid]['name']
            
            # ã“ã“ã§æœ¬æ¥ã¯LLMã«ã€Œã“ã®åŸºæº–ã§æ¡ç‚¹ã—ã¦ã€ã¨æŠ•ã’ã¾ã™
            # ä»Šå›ã¯ãƒ©ãƒ³ãƒ€ãƒ ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            score = random.randint(60, 100)
            
            print(f"   - {criteria_name}: {score}ç‚¹")
            
            # ã‚‚ã—åŸºæº–ãŒã€Œå½¢å¼éµå®ˆã€ãªã®ã«ç‚¹æ•°ãŒä½ã‹ã£ãŸã‚‰ã€å³ã—ã„ã‚³ãƒ¡ãƒ³ãƒˆã‚’å‡ºã™ãªã©
            if score < 70:
                print(f"     âš ï¸ è­¦å‘Š: {criteria_name} ã®åŸºæº–ã‚’æº€ãŸã—ã¦ã„ã¾ã›ã‚“ã€‚æ”¹å–„ãŒå¿…è¦ã§ã™ã€‚")
            
            total_score += score
            report[cid] = score

        avg_score = total_score / len(active_criteria_ids)
        print(f"\nğŸ“Š ç·åˆè©•ä¾¡: {avg_score:.1f}ç‚¹")
        
        return {
            "average_score": avg_score,
            "detailed_scores": report,
            "criteria_used": active_criteria_ids,
            "is_passed": avg_score >= 75
        }

    def evaluate_with_llm(self, prompt_text: str, ai_output: str) -> Dict:
        """
        å®Ÿéš›ã®LLMã‚’ä½¿ç”¨ã—ã¦è©•ä¾¡ã™ã‚‹ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå®Ÿè£…ã™ã‚‹ï¼‰
        
        ä¾‹:
        import openai
        
        # 1. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ„å›³ã‚’åˆ†æ
        criteria_ids = self.analyzer.analyze_intent(prompt_text)
        
        # 2. å„åŸºæº–ã§è©•ä¾¡
        for cid in criteria_ids:
            criteria_desc = self.analyzer.criteria_lib[cid]['description']
            evaluation_prompt = f'''
            ä»¥ä¸‹ã®å‡ºåŠ›ã‚’ã€Œ{criteria_desc}ã€ã®è¦³ç‚¹ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
            
            å‡ºåŠ›:
            {ai_output}
            
            0-100ç‚¹ã§ã‚¹ã‚³ã‚¢ã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚
            '''
            
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[{"role": "user", "content": evaluation_prompt}]
            )
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦æ¡ç‚¹
        """
        raise NotImplementedError(
            "å®Ÿéš›ã®LLMè©•ä¾¡ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ã“ã®é–¢æ•°ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚\n"
            "ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€evaluate_prompt_effectiveness()ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
        )


# ============================================================
# ğŸŒŸ GEMINI API INTEGRATION (æœ¬ç‰©ã®LLMè©•ä¾¡)
# ============================================================

# Gemini APIã®åˆæœŸåŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
_gemini_model = None

def _initialize_gemini():
    """Gemini APIã‚’åˆæœŸåŒ–ã™ã‚‹"""
    global _gemini_model
    
    if _gemini_model is not None:
        return _gemini_model
    
    try:
        import os
        import google.generativeai as genai
        
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
        api_key = os.environ.get('GEMINI_API_KEY')
        
        if not api_key:
            print("âš ï¸ GEMINI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            print("   ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™ã€‚")
            return None
        
        genai.configure(api_key=api_key)
        _gemini_model = genai.GenerativeModel('gemini-1.5-flash')
        print("âœ… Gemini APIãŒåˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸã€‚")
        return _gemini_model
        
    except ImportError:
        print("âš ï¸ google-generativeaiãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print("   pip install google-generativeai ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return None
    except Exception as e:
        print(f"âŒ Gemini APIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return None


class GeminiIntentAnalyzer:
    """
    Gemini APIã‚’ä½¿ç”¨ã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ„å›³ã‚’æ·±ãç†è§£ã™ã‚‹
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã§ã¯ãªãã€æ–‡è„ˆç†è§£ã«ã‚ˆã‚‹è©•ä¾¡åŸºæº–é¸æŠ
    """
    
    def __init__(self, criteria_lib: Dict):
        self.criteria_lib = criteria_lib
        self.model = _initialize_gemini()

    def analyze_intent(self, prompt_text: str) -> List[str]:
        """
        Gemini APIã‚’ä½¿ã£ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ„å›³ã‚’åˆ†æã—ã€
        é©åˆ‡ãªè©•ä¾¡åŸºæº–IDã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        """
        import json
        import re
        
        print("ğŸ§  --- GeminiãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã€æ„å›³ã€ã‚’èª­è§£ä¸­... ---")
        
        # APIãŒä½¿ãˆãªã„å ´åˆã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if self.model is None:
            print("   âš ï¸ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œä¸­")
            return self._simulate_analyze(prompt_text)
        
        # è©•ä¾¡åŸºæº–ãƒªã‚¹ãƒˆã‚’AIã«æ¸¡ã™ãŸã‚ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
        criteria_list_text = "\n".join([
            f"- ID: {k} | åŸºæº–å: {v['name']} | èª¬æ˜: {v['description']}"
            for k, v in self.criteria_lib.items()
        ])

        # Geminiã¸ã®æŒ‡ç¤ºï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
        system_instruction = f"""
ã‚ãªãŸã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ›¸ã„ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ã‚’åˆ†æã—ã€ãã®å“è³ªã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«é©åˆ‡ãªã€Œè©•ä¾¡åŸºæº–IDã€ã‚’ãƒªã‚¹ãƒˆã‹ã‚‰é¸ã‚“ã§ãã ã•ã„ã€‚

ã€è©•ä¾¡åŸºæº–ãƒªã‚¹ãƒˆã€‘
{criteria_list_text}

ã€ãƒ«ãƒ¼ãƒ«ã€‘
1. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç›®çš„ï¼ˆå‰µé€ çš„ãªã®ã‹ã€è«–ç†çš„ãªã®ã‹ã€ãªã©ï¼‰ã‚’æ·±ãèª­ã¿å–ã‚‹ã“ã¨ã€‚
2. æœ€ã‚‚é‡è¦ã¨æ€ã‚ã‚Œã‚‹åŸºæº–ã‚’1ã¤ã€œ3ã¤é¸ã¶ã“ã¨ã€‚
3. å‡ºåŠ›ã¯ã€é¸ã‚“ã IDã®ãƒªã‚¹ãƒˆï¼ˆJSONå½¢å¼ï¼‰ã®ã¿ã‚’è¿”ã™ã“ã¨ã€‚ä½™è¨ˆãªè§£èª¬ã¯ä¸è¦ã€‚
ä¾‹: ["accuracy", "format_compliance"]
"""

        try:
            # Geminiã«å•ã„åˆã‚ã›
            response = self.model.generate_content(
                f"{system_instruction}\n\nã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‘\n{prompt_text}"
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONéƒ¨åˆ†ã‚’æŠ½å‡º
            json_text = re.search(r'\[.*\]', response.text, re.DOTALL).group()
            selected_ids = json.loads(json_text)
            
            # IDãŒãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            valid_ids = [pid for pid in selected_ids if pid in self.criteria_lib]
            
            if not valid_ids:
                return ["accuracy"]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            
            print(f"   ğŸ“‹ é¸æŠã•ã‚ŒãŸè©•ä¾¡åŸºæº–: {[self.criteria_lib[cid]['name'] for cid in valid_ids]}")
            return valid_ids

        except Exception as e:
            print(f"   âŒ APIã‚¨ãƒ©ãƒ¼: {e}")
            return ["accuracy"]  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯æœ€ä½é™ã®åŸºæº–ã‚’è¿”ã™

    def _simulate_analyze(self, prompt_text: str) -> List[str]:
        """ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°"""
        selected_ids = []
        
        for c_id, c_data in self.criteria_lib.items():
            for keyword in c_data['keywords']:
                if keyword in prompt_text:
                    selected_ids.append(c_id)
                    break
        
        if not selected_ids:
            selected_ids = ["accuracy"]
        
        selected_ids = list(set(selected_ids))
        print(f"   ğŸ“‹ é¸æŠã•ã‚ŒãŸè©•ä¾¡åŸºæº–: {[self.criteria_lib[cid]['name'] for cid in selected_ids]}")
        return selected_ids


class GeminiEvaluator:
    """
    Gemini APIã‚’ä½¿ç”¨ã—ã¦AIã®å‡ºåŠ›ã‚’æœ¬æ ¼çš„ã«è©•ä¾¡ã™ã‚‹
    JSONå½¢å¼ã§è©³ç´°ãªæ¡ç‚¹çµæœã‚’è¿”ã™
    """
    
    def __init__(self, analyzer: GeminiIntentAnalyzer):
        self.analyzer = analyzer
        self.model = _initialize_gemini()

    def evaluate(self, prompt_text: str, ai_output: str) -> Dict:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¦‹ã¦è©•ä¾¡åŸºæº–ã‚’æ±ºã‚ã€ãã®åŸºæº–ã§å‡ºåŠ›ã‚’æ¡ç‚¹ã™ã‚‹
        """
        import json
        import re
        
        # 1. æ„å›³ã‚’ç†è§£ã™ã‚‹ï¼ˆGeminiIntentAnalyzerã‚’ä½¿ç”¨ï¼‰
        active_criteria_ids = self.analyzer.analyze_intent(prompt_text)
        
        # åŸºæº–ã®åå‰ãƒªã‚¹ãƒˆã‚’å–å¾—
        active_criteria_names = [
            self.analyzer.criteria_lib[cid]['name'] for cid in active_criteria_ids
        ]
        print(f"\nğŸ¯ AIãŒæ±ºå®šã—ãŸè©•ä¾¡è»¸: {active_criteria_names}")
        
        # APIãŒä½¿ãˆãªã„å ´åˆã¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        if self.model is None:
            print("   âš ï¸ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œä¸­")
            return self._simulate_evaluate(active_criteria_ids, active_criteria_names)

        # 2. æ¡ç‚¹ã‚’è¡Œã†
        print("ğŸ“ --- Geminiå¯©æŸ»å“¡ãŒæ¡ç‚¹ä¸­... ---")
        
        # æ¡ç‚¹ç”¨ã®æŒ‡ç¤º
        scoring_instruction = f"""
ä»¥ä¸‹ã®ã€Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ã¨ã€ãã‚Œã«å¯¾ã™ã‚‹ã€ŒAIã®å‡ºåŠ›ã€ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚
è©•ä¾¡ã¯ã€ä»¥ä¸‹ã®ã€Œé‡ç‚¹è©•ä¾¡é …ç›®ã€ã«åŸºã¥ã„ã¦å³å¯†ã«è¡Œã£ã¦ãã ã•ã„ã€‚

ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæŒ‡ç¤ºï¼‰ã€‘
{prompt_text}

ã€AIã®å‡ºåŠ›ï¼ˆè©•ä¾¡å¯¾è±¡ï¼‰ã€‘
{ai_output}

ã€é‡ç‚¹è©•ä¾¡é …ç›®ã€‘
{", ".join(active_criteria_names)}

ã€å‡ºåŠ›å½¢å¼ã€‘
ä»¥ä¸‹ã®JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
{{
    "total_score": 0ã€œ100ã®æ•´æ•°,
    "feedback": "æ”¹å–„ç‚¹ã‚„è‰¯ã‹ã£ãŸç‚¹ã«ã¤ã„ã¦ã®å…·ä½“çš„ãªã‚³ãƒ¡ãƒ³ãƒˆï¼ˆ100æ–‡å­—ç¨‹åº¦ï¼‰",
    "details": {{
        "åŸºæº–å1": ç‚¹æ•°,
        "åŸºæº–å2": ç‚¹æ•°
    }}
}}
"""
        try:
            response = self.model.generate_content(scoring_instruction)
            
            # JSONè§£æ
            json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                
                print(f"\nğŸ“Š ç·åˆã‚¹ã‚³ã‚¢: {result['total_score']}ç‚¹")
                print(f"ğŸ’¬ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯: {result['feedback']}")
                print("ğŸ“‹ è©³ç´°ã‚¹ã‚³ã‚¢:")
                for k, v in result.get("details", {}).items():
                    print(f"   - {k}: {v}ç‚¹")
                
                result['criteria_used'] = active_criteria_ids
                result['is_passed'] = result['total_score'] >= 75
                return result
            else:
                print("âŒ è§£æã‚¨ãƒ©ãƒ¼: JSONãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return {"total_score": 0, "is_passed": False}

        except Exception as e:
            print(f"âŒ APIã‚¨ãƒ©ãƒ¼: {e}")
            return {"total_score": 0, "is_passed": False}

    def _simulate_evaluate(self, criteria_ids: List[str], criteria_names: List[str]) -> Dict:
        """ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ©ãƒ³ãƒ€ãƒ è©•ä¾¡"""
        print("ğŸ“ --- æ¡ç‚¹é–‹å§‹ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰---")
        
        total_score = 0
        details = {}
        
        for name in criteria_names:
            score = random.randint(60, 100)
            details[name] = score
            total_score += score
            print(f"   - {name}: {score}ç‚¹")
        
        avg_score = total_score / len(criteria_names)
        
        print(f"\nğŸ“Š ç·åˆã‚¹ã‚³ã‚¢: {avg_score:.0f}ç‚¹")
        
        return {
            "total_score": round(avg_score),
            "feedback": "ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§è©•ä¾¡ã—ã¾ã—ãŸã€‚",
            "details": details,
            "criteria_used": criteria_ids,
            "is_passed": avg_score >= 75
        }


# ============================================================
# ğŸ”„ OPTIMIZATION LOOP (è‡ªå‹•æœ€é©åŒ–ãƒ«ãƒ¼ãƒ—)
# ============================================================

class GeminiRefiner:
    """
    ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ã„ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è‡ªå‹•çš„ã«æ›¸ãç›´ã™
    """
    
    def __init__(self, model=None):
        self.model = model if model else _initialize_gemini()

    def refine_prompt(self, current_prompt: str, feedback: str, criteria_names: List[str]) -> str:
        """
        è©•ä¾¡ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ã„ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ”¹å–„ã™ã‚‹
        """
        print("ğŸ”§ --- RefinerãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¿®æ­£ä¸­... ---")
        
        if self.model is None:
            print("   âš ï¸ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œä¸­")
            return self._simulate_refine(current_prompt, feedback)

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ›¸ãç›´ã•ã›ã‚‹ãŸã‚ã®ãƒ¡ã‚¿ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        instruction = f"""
ã‚ãªãŸã¯ä¸–ç•Œæœ€é«˜å³°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚
ä»¥ä¸‹ã®ã€Œç¾åœ¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ã‚’ä½¿ã£ã¦AIã«æŒ‡ç¤ºã‚’å‡ºã—ã¾ã—ãŸãŒã€å“è³ªãƒã‚§ãƒƒã‚¯ã§ã€Œæ”¹å–„ãŒå¿…è¦ã€ã¨åˆ¤å®šã•ã‚Œã¾ã—ãŸã€‚

ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼ˆæ”¹å–„ç‚¹ï¼‰ã€‘
{feedback}

ã€é‡è¦–ã™ã¹ãè©•ä¾¡åŸºæº–ã€‘
{", ".join(criteria_names)}

ã€ç¾åœ¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‘
{current_prompt}

ã€æŒ‡ç¤ºã€‘
ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®å†…å®¹ã‚’åæ˜ ã—ã€è©•ä¾¡åŸºæº–ã‚’æº€ãŸã™ã‚ˆã†ã«ã€Œç¾åœ¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ã‚’æ›¸ãç›´ã—ã¦ãã ã•ã„ã€‚
å…ƒã®æ„å›³ã¯å´©ã•ãšã€æŒ‡ç¤ºã‚’å…·ä½“åŒ–ãƒ»æ˜ç¢ºåŒ–ãƒ»å¼·åŒ–ã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›ã¯ã€Œä¿®æ­£å¾Œã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ¬æ–‡ã®ã¿ã€ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚æŒ¨æ‹¶ã‚„è§£èª¬ã¯ä¸è¦ã§ã™ã€‚
"""
        try:
            response = self.model.generate_content(instruction)
            new_prompt = response.text.strip()
            print("âœ¨ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæ›¸ãç›´ã•ã‚Œã¾ã—ãŸï¼")
            return new_prompt
        except Exception as e:
            print(f"âŒ Refinerã‚¨ãƒ©ãƒ¼: {e}")
            return current_prompt  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãã®ã¾ã¾è¿”ã™

    def _simulate_refine(self, current_prompt: str, feedback: str) -> str:
        """ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ”¹å–„"""
        improved = f"{current_prompt}\n\nã€æ”¹å–„é©ç”¨ã€‘{feedback}ã«åŸºã¥ã„ã¦å…·ä½“åŒ–ã—ã¾ã—ãŸã€‚"
        print("âœ¨ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæ›¸ãç›´ã•ã‚Œã¾ã—ãŸï¼ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")
        return improved


class GeminiGenerator:
    """
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ã„ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã™ã‚‹
    """
    
    def __init__(self, model=None):
        self.model = model if model else _initialize_gemini()

    def generate(self, prompt: str) -> str:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ã„ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ
        """
        print("ğŸ¤– --- GeneratorãŒã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œä¸­... ---")
        
        if self.model is None:
            print("   âš ï¸ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œä¸­")
            return self._simulate_generate(prompt)
        
        try:
            response = self.model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            return f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}"

    def _simulate_generate(self, prompt: str) -> str:
        """ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ"""
        return f"ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: ã€Œ{prompt[:30]}...ã€ã«å¯¾ã™ã‚‹ç”Ÿæˆçµæœï¼‰"


def run_optimization_loop(
    initial_prompt: str, 
    criteria_lib: Dict,
    max_retries: int = 3, 
    passing_score: int = 80
) -> Dict:
    """
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è‡ªå‹•çš„ã«æœ€é©åŒ–ã™ã‚‹ãƒ«ãƒ¼ãƒ—
    
    Args:
        initial_prompt: åˆæœŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        criteria_lib: è©•ä¾¡åŸºæº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆCRITERIA_LIBRARYï¼‰
        max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
        passing_score: åˆæ ¼ç‚¹ï¼ˆã“ã‚Œã‚’è¶…ãˆãŸã‚‰çµ‚äº†ï¼‰
        
    Returns:
        æœ€çµ‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€å‡ºåŠ›ã€ã‚¹ã‚³ã‚¢ã‚’å«ã‚€è¾æ›¸
    """
    # å„å½¹å‰²ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    analyzer = GeminiIntentAnalyzer(criteria_lib)
    evaluator = GeminiEvaluator(analyzer)
    refiner = GeminiRefiner()
    generator = GeminiGenerator()

    current_prompt = initial_prompt
    
    print(f"\nğŸ æœ€é©åŒ–ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆæœ€å¤§ {max_retries} å›ï¼‰")
    print(f"   åˆæ ¼ç‚¹: {passing_score}ç‚¹\n")

    for i in range(1, max_retries + 1):
        print(f"\n{'='*60}")
        print(f"ğŸ”„ --- ãƒ©ã‚¦ãƒ³ãƒ‰ {i}/{max_retries} ---")
        print(f"{'='*60}")
        
        # 1. ç”Ÿæˆ (Generate)
        output = generator.generate(current_prompt)
        print(f"\nğŸ“„ [AIã®ç”Ÿæˆçµæœ (æŠœç²‹)]:\n{output[:100]}...\n")
        
        # 2. è©•ä¾¡ (Evaluate)
        eval_result = evaluator.evaluate(current_prompt, output)
        score = eval_result.get("total_score", 0)
        feedback = eval_result.get("feedback", "ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãªã—")
        
        # 3. åˆ¤å®š (Check)
        if score >= passing_score:
            print(f"\nğŸ‰ åˆæ ¼ç‚¹({passing_score}ç‚¹)ã‚’è¶…ãˆã¾ã—ãŸï¼ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            return {
                "final_prompt": current_prompt,
                "final_output": output,
                "score": score,
                "rounds": i,
                "status": "passed"
            }
        
        # 4. ä¿®æ­£ (Refine) - æœ€çµ‚ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãªã‘ã‚Œã°å®Ÿè¡Œ
        if i < max_retries:
            print(f"\nâš ï¸ ã‚¹ã‚³ã‚¢ä¸è¶³({score}ç‚¹ < {passing_score}ç‚¹)ã€‚")
            print("   ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ã„ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¿®æ­£ã—ã¾ã™ã€‚")
            
            # è©•ä¾¡ã«ä½¿ã‚ã‚ŒãŸåŸºæº–åã‚’å–å¾—
            active_ids = eval_result.get("criteria_used", ["accuracy"])
            criteria_names = [criteria_lib[cid]['name'] for cid in active_ids if cid in criteria_lib]
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ›¸ãæ›ãˆ
            current_prompt = refiner.refine_prompt(current_prompt, feedback, criteria_names)
            
            print(f"\nğŸ“ [ä¿®æ­£å¾Œã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ]:\n{current_prompt[:200]}...")
        else:
            print(f"\nğŸ›‘ æœ€å¤§è©¦è¡Œå›æ•°({max_retries}å›)ã«é”ã—ã¾ã—ãŸã€‚")

    return {
        "final_prompt": current_prompt,
        "final_output": output,
        "score": score,
        "rounds": max_retries,
        "status": "max_retries_reached"
    }


# ============================================================
# ğŸ§¬ DATA SYNTHESIS (ãƒ‡ãƒ¼ã‚¿åˆæˆ)
# ============================================================

class DataSynthesizer:
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³ã«åˆã‚ã›ã¦ã€é«˜å“è³ªãªã€Œä¾‹ï¼ˆFew-Shotï¼‰ã€ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹
    """
    
    def __init__(self, model=None):
        self.model = model if model else _initialize_gemini()

    def generate_examples(self, intent: str, count: int = 3) -> str:
        """
        æ„å›³ã«åŸºã¥ã„ã¦Few-Shotã®æˆåŠŸäº‹ä¾‹ã‚’è‡ªå‹•ç”Ÿæˆ
        
        Args:
            intent: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³
            count: ç”Ÿæˆã™ã‚‹ä¾‹ã®æ•°
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸä¾‹ã®ãƒ†ã‚­ã‚¹ãƒˆ
        """
        print(f"ğŸ§¬ --- ãƒ‡ãƒ¼ã‚¿åˆæˆä¸­: ã€Œ{intent}ã€ã®æˆåŠŸäº‹ä¾‹ã‚’ä½œã£ã¦ã„ã¾ã™... ---")
        
        if self.model is None:
            print("   âš ï¸ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œä¸­")
            return self._simulate_examples(intent, count)
        
        prompt = f"""
ã‚ãªãŸã¯å„ªç§€ãªãƒ‡ãƒ¼ã‚¿ä½œæˆè€…ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³ã€Œ{intent}ã€ã‚’é”æˆã™ã‚‹ãŸã‚ã®ã€ç†æƒ³çš„ãªã€Œå…¥åŠ›ã¨å‡ºåŠ›ã®ä¾‹ã€ã‚’{count}ã¤ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€è¦ä»¶ã€‘
1. å…·ä½“çš„ã§å®Ÿç”¨çš„ãªå†…å®¹ã«ã™ã‚‹ã“ã¨ã€‚
2. è‰¯ã„ä¾‹ï¼ˆGood Exampleï¼‰ã§ã‚ã‚‹ã“ã¨ã€‚
3. å‡ºåŠ›å½¢å¼ã¯ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ã¿ã€‚

Example 1:
Input: ...
Output: ...

Example 2:
...
"""
        try:
            response = self.model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            return f"ä¾‹ã®ç”Ÿæˆã«å¤±æ•—: {e}"

    def _simulate_examples(self, intent: str, count: int) -> str:
        """ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ä¾‹ç”Ÿæˆ"""
        examples = []
        for i in range(1, count + 1):
            examples.append(f"""Example {i}:
Input: {intent}ã«é–¢ã™ã‚‹å…¥åŠ›ä¾‹{i}
Output: {intent}ã‚’é”æˆã™ã‚‹å‡ºåŠ›ä¾‹{i}""")
        return "\n\n".join(examples)


class CasualTextRefiner:
    """
    è©±ã—è¨€è‘‰ï¼ˆã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ï¼‰ã‚’ã€ç›®çš„ã«åˆã‚ã›ã¦æœ€é©åŒ–ã™ã‚‹å¤‰æ›å™¨
    """
    
    def __init__(self, model=None, synthesizer: DataSynthesizer = None):
        self.model = model if model else _initialize_gemini()
        self.synthesizer = synthesizer if synthesizer else DataSynthesizer(self.model)

    def refine(self, user_text: str, style: str, use_few_shot: bool = True) -> str:
        """
        ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šã•ã‚ŒãŸã‚¹ã‚¿ã‚¤ãƒ«ã«å¤‰æ›
        
        Args:
            user_text: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªå…¥åŠ›
            style: å¤‰æ›å…ˆã®ã‚¹ã‚¿ã‚¤ãƒ«
            use_few_shot: Few-Shotãƒ‡ãƒ¼ã‚¿åˆæˆã‚’ä½¿ç”¨ã™ã‚‹ã‹
            
        Returns:
            å¤‰æ›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        if self.model is None:
            print("   âš ï¸ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œä¸­")
            return self._simulate_refine(user_text, style)
        
        # 1. ã¾ãšã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€Œæ„å›³ã€ã‚’èª­ã¿å–ã‚‹
        print("ğŸ¯ --- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³ã‚’åˆ†æä¸­... ---")
        intent_prompt = f"ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®ã€ç›®çš„ã€ã‚’5æ–‡å­—ã€œ20æ–‡å­—ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚\nãƒ†ã‚­ã‚¹ãƒˆ: {user_text}"
        
        try:
            intent_resp = self.model.generate_content(intent_prompt)
            intent = intent_resp.text.strip()
            print(f"   èª­ã¿å–ã£ãŸæ„å›³: {intent}")
        except Exception as e:
            print(f"   âŒ æ„å›³åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            intent = "ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–"
        
        # 2. ãã®æ„å›³ã«åˆã£ãŸã€ŒæˆåŠŸäº‹ä¾‹ã€ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ (Few-Shot Data Synthesis)
        examples = ""
        if use_few_shot:
            examples = self.synthesizer.generate_examples(intent)
        
        # 3. äº‹ä¾‹ã‚’ä½¿ã£ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å¤‰æ›ã™ã‚‹ (ICL)
        print("âœï¸ --- æœ¬ç•ªæ›¸ãè¾¼ã¿ä¸­... ---")
        conversion_prompt = f"""
ã‚ãªãŸã¯ãƒ—ãƒ­ã®ãƒ©ã‚¤ã‚¿ãƒ¼/ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œãƒ©ãƒ•ãªå…¥åŠ›ã€ã‚’ã€æŒ‡å®šã•ã‚ŒãŸã€Œã‚¹ã‚¿ã‚¤ãƒ«ã€ã«åˆã‚ã›ã¦æ›¸ãç›´ã—ã¦ãã ã•ã„ã€‚
{"ä»¥ä¸‹ã®ã€ŒæˆåŠŸäº‹ä¾‹ã€ã‚’å‚è€ƒã«ã€å“è³ªã‚’é«˜ã‚ã¦ãã ã•ã„ã€‚" if examples else ""}

{"ã€æˆåŠŸäº‹ä¾‹ (Few-Shot Data)ã€‘" + chr(10) + examples if examples else ""}

ã€ã‚¹ã‚¿ã‚¤ãƒ«æŒ‡å®šã€‘
{style}

ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ©ãƒ•ãªå…¥åŠ›ã€‘
{user_text}

ã€å‡ºåŠ›ã€‘
æ›¸ãç›´ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
        try:
            response = self.model.generate_content(conversion_prompt)
            result = response.text.strip()
            print("âœ… ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›å®Œäº†ï¼")
            return result
        except Exception as e:
            return f"å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}"

    def _simulate_refine(self, user_text: str, style: str) -> str:
        """ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›"""
        return f"ã€{style}ã‚¹ã‚¿ã‚¤ãƒ«ã«å¤‰æ›ã€‘\n{user_text}"
