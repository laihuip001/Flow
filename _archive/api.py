"""
AI Clipboard API - FastAPI REST Server
ã‚¹ãƒãƒ›ã‚¢ãƒ—ãƒªã‚„å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹ã‹ã‚‰å‘¼ã³å‡ºã›ã‚‹APIã‚µãƒ¼ãƒãƒ¼
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Optional, List
import os

# --- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š ---
app = FastAPI(
    title="AI Clipboard API",
    description="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®REST API",
    version="1.0.0"
)

# CORSè¨­å®šï¼ˆã‚¯ãƒ­ã‚¹ã‚ªãƒªã‚¸ãƒ³ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¨±å¯ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Gemini APIè¨­å®š ---
model = None

def get_model():
    """Gemini ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ï¼ˆé…å»¶åˆæœŸåŒ–ï¼‰"""
    global model
    if model is not None:
        return model
    
    api_key = os.environ.get('GEMINI_API_KEY')
    if not api_key:
        return None
    
    try:
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')
        return model
    except Exception as e:
        print(f"Gemini APIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return None


# --- ãƒ­ã‚¸ãƒƒã‚¯ã‚¯ãƒ©ã‚¹ ---
class DataSynthesizer:
    """Few-Shotãƒ‡ãƒ¼ã‚¿åˆæˆ"""
    def __init__(self, model):
        self.model = model
        
    def generate_examples(self, intent: str, count: int = 3) -> str:
        prompt = f"""ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³ã€Œ{intent}ã€ã‚’é”æˆã™ã‚‹ãŸã‚ã®ã€ç†æƒ³çš„ãªã€Œå…¥åŠ›ã¨å‡ºåŠ›ã®ä¾‹ã€ã‚’{count}ã¤ä½œæˆã›ã‚ˆã€‚
Example 1:
Input: ...
Output: ..."""
        try:
            return self.model.generate_content(prompt).text.strip()
        except:
            return ""


# --- ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‹å®šç¾© ---
class RefineRequest(BaseModel):
    """ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    text: str = Field(..., description="å¤‰æ›ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ")
    style: str = Field(default="ãƒ“ã‚¸ãƒã‚¹ (ä¸å¯§)", description="å¤‰æ›ã‚¹ã‚¿ã‚¤ãƒ«")
    use_few_shot: bool = Field(default=True, description="Few-Shotåˆæˆã‚’ä½¿ç”¨ã™ã‚‹ã‹")

class RefineResponse(BaseModel):
    """ãƒ†ã‚­ã‚¹ãƒˆæœ€é©åŒ–ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    result: str
    intent: str
    style: str

class StatusResponse(BaseModel):
    """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    status: str
    message: str
    api_configured: bool

class StylesResponse(BaseModel):
    """åˆ©ç”¨å¯èƒ½ãªã‚¹ã‚¿ã‚¤ãƒ«"""
    styles: List[str]


# --- APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ---

@app.get("/", response_model=StatusResponse)
def read_root():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return StatusResponse(
        status="ok",
        message="AI Clipboard API is running!",
        api_configured=get_model() is not None
    )

@app.get("/styles", response_model=StylesResponse)
def get_styles():
    """åˆ©ç”¨å¯èƒ½ãªå¤‰æ›ã‚¹ã‚¿ã‚¤ãƒ«ã‚’å–å¾—"""
    return StylesResponse(styles=[
        "ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒ¼ãƒ« (è¬ç½ªãƒ»ä¾é ¼)",
        "ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢å‘ã‘ (è¦ä»¶å®šç¾©)",
        "SNSæŠ•ç¨¿ (è¦ªã—ã¿ã‚„ã™ã)",
        "è‹±èªç¿»è¨³ (ãƒ“ã‚¸ãƒã‚¹)",
        "è«–ç†çš„è¦ç´„ (ç®‡æ¡æ›¸ã)",
        "ä¸å¯§ãªã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆ",
        "æŠ€è¡“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ"
    ])

@app.post("/refine", response_model=RefineResponse)
async def refine_text(body: RefineRequest):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šã•ã‚ŒãŸã‚¹ã‚¿ã‚¤ãƒ«ã«æœ€é©åŒ–"""
    print(f"ğŸ“© å—ä¿¡: {body.text[:30]}...")
    
    current_model = get_model()
    if current_model is None:
        raise HTTPException(
            status_code=503,
            detail="Gemini APIãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚GEMINI_API_KEYç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        )
    
    try:
        # 1. æ„å›³ç†è§£
        intent_resp = current_model.generate_content(
            f"ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®ç›®çš„ã‚’ç°¡æ½”ã«è¦ç´„ã›ã‚ˆ: {body.text}"
        )
        intent = intent_resp.text.strip()
        
        # 2. äº‹ä¾‹ç”Ÿæˆ (Data Synthesis)
        examples = ""
        if body.use_few_shot:
            synthesizer = DataSynthesizer(current_model)
            examples = synthesizer.generate_examples(intent)
        
        # 3. æœ€é©åŒ–å®Ÿè¡Œ
        prompt = f"""
ã‚ãªãŸã¯ãƒ—ãƒ­ã®ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®ã€Œãƒ©ãƒ•ãªå…¥åŠ›ã€ã‚’ã€Œ{body.style}ã€ã«åˆã‚ã›ã¦æ›¸ãç›´ã—ã¦ãã ã•ã„ã€‚
{"ä»¥ä¸‹ã®ã€ŒæˆåŠŸäº‹ä¾‹ã€ã‚’å‚è€ƒã«ã€ã‚¯ã‚ªãƒªãƒ†ã‚£ã‚’é«˜ã‚ã¦ãã ã•ã„ã€‚" if examples else ""}

{"ã€æˆåŠŸäº‹ä¾‹ã€‘" + chr(10) + examples if examples else ""}

ã€å…¥åŠ›ã€‘
{body.text}

ã€å‡ºåŠ›ã€‘
æ›¸ãç›´ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚ï¼ˆè§£èª¬ä¸è¦ï¼‰
"""
        response = current_model.generate_content(prompt)
        result_text = response.text.strip()
        
        print(f"ğŸ“¤ è¿”ä¿¡: {result_text[:30]}...")
        
        return RefineResponse(
            result=result_text,
            intent=intent,
            style=body.style
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# --- Filter (ä¸‹å‡¦ç†) ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/ãƒ¬ã‚¹ãƒãƒ³ã‚¹ ---
class FilterRequest(BaseModel):
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¸‹å‡¦ç†ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    text: str = Field(..., description="ä¸‹å‡¦ç†ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ")
    mode: str = Field(default="raw", description="raw/light (æ•´å½¢), heavy (æ§‹é€ åŒ–), deep (é€†è³ªå•)")
    context: str = Field(default="", description="deepãƒ¢ãƒ¼ãƒ‰ã§é€†è³ªå•ã¸ã®å›ç­”ã‚’å«ã‚ã‚‹")

class FilterResponse(BaseModel):
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¸‹å‡¦ç†ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    type: str = Field(..., description="complete (å®Œäº†) or question (è¿½åŠ è³ªå•)")
    text: str
    mode: str


def process_prompt(text: str, mode: str, context: str, current_model) -> dict:
    """
    ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å‡¦ç†ã™ã‚‹
    
    - raw/light: é«˜é€Ÿæ•´å½¢ï¼ˆãƒã‚¤ã‚ºé™¤å»ã®ã¿ï¼‰
    - heavy: å®Œå…¨æ§‹é€ åŒ–ï¼ˆ6W3Hè£œå®Œï¼‰
    - deep: é€†è³ªå•ã§ä¸è¶³æƒ…å ±ã‚’è£œå®Œ
    """
    import google.generativeai as genai
    import json
    
    # -------------------------------------------------
    # 1. ã€Raw / Lightã€‘: é«˜é€Ÿæ•´å½¢
    # -------------------------------------------------
    if mode in ["raw", "light"]:
        system_prompt = """
ã‚ãªãŸã¯ã€Œãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢ãƒ„ãƒ¼ãƒ«ã€ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¸ã®æŒ‡ç¤ºã¨ã—ã¦é•å’Œæ„ŸãŒãªã„ã‚ˆã†ã«ã€Œä½“è£ã€ã ã‘ã‚’æ•´ãˆã¦ãã ã•ã„ã€‚

ã€ãƒ«ãƒ¼ãƒ«ã€‘
1. æ–‡æ„ã€ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ã¯å¯èƒ½ãªé™ã‚Šç¶­æŒã™ã‚‹ï¼ˆå‹æ‰‹ã«è£œå®Œã—ãªã„ï¼‰ã€‚
2. ã€Œãˆãƒ¼ã¨ã€ãªã©ã®æ˜ã‚‰ã‹ãªãƒã‚¤ã‚ºã®ã¿å‰Šé™¤ã™ã‚‹ã€‚
3. èªå°¾ã‚’ã€Œã€œã—ã¦ãã ã•ã„ã€ã€Œã€œã›ã‚ˆã€ç­‰ã®å‘½ä»¤å½¢ã€ã¾ãŸã¯ä½“è¨€æ­¢ã‚ã«çµ±ä¸€ã™ã‚‹ã€‚
4. ãƒ­ãƒ¼ãƒ«ï¼ˆäººæ ¼ï¼‰ã®ä»˜ä¸ã¯ã—ãªã„ã€‚

å‡ºåŠ›ã¯æ•´å½¢å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã€‚
"""
        user_prompt = f"ã€å…¥åŠ›ã€‘\n{text}\n\nã€æ•´å½¢å¾Œã€‘"
        
        try:
            response = current_model.generate_content(
                system_prompt + user_prompt,
                generation_config=genai.types.GenerationConfig(temperature=0.1)
            )
            return {"type": "complete", "text": response.text.strip()}
        except Exception as e:
            return {"type": "error", "text": str(e)}

    # -------------------------------------------------
    # 2. ã€Heavy / Structureã€‘: ã‚¬ãƒƒãƒ„ãƒªè£œå®Œãƒ»æ§‹é€ åŒ–
    # -------------------------------------------------
    elif mode == "heavy":
        system_prompt = """
ã‚ãªãŸã¯ã€ŒæŒ‡ç¤ºæ§‹é€ åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã€ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’ã€AIãŒèª¤è§£ãªãå®Ÿè¡Œã§ãã‚‹ã€Œå®Œç’§ãªä»•æ§˜æ›¸ã€ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚

ã€ãƒ«ãƒ¼ãƒ«ã€‘
1. 6W3Hï¼ˆèª°ãŒã€ã„ã¤ã€ã©ã“ã§ã€ä½•ã‚’ã€ãªãœã€ã©ã®ã‚ˆã†ã«...ï¼‰ã‚’æ¨è«–ã—ã€ä¸è¶³ãŒã‚ã‚Œã°å¸¸è­˜ã®ç¯„å›²ã§è£œå®Œã™ã‚‹ã€‚
2. ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã§è¦‹å‡ºã—ã‚’ä»˜ã‘ã€è¦–èªæ€§ã‚’é«˜ã‚ã‚‹ã€‚
3. ãƒ­ãƒ¼ãƒ«ï¼ˆäººæ ¼ï¼‰ã®ä»˜ä¸ã¯ã—ãªã„ã€‚
4. æŠ½è±¡çš„ãªè¡¨ç¾ã¯å…·ä½“çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¤‰æ›ã™ã‚‹ã€‚

å‡ºåŠ›ã¯æ§‹é€ åŒ–ã•ã‚ŒãŸæŒ‡ç¤ºã®ã¿ã€‚
"""
        user_prompt = f"ã€å…¥åŠ›ã€‘\n{text}\n\nã€æ§‹é€ åŒ–ã•ã‚ŒãŸæŒ‡ç¤ºã€‘"
        
        try:
            response = current_model.generate_content(
                system_prompt + user_prompt,
                generation_config=genai.types.GenerationConfig(temperature=0.2)
            )
            return {"type": "complete", "text": response.text.strip()}
        except Exception as e:
            return {"type": "error", "text": str(e)}

    # -------------------------------------------------
    # 3. ã€Deep Researchã€‘: ä¸è¶³æƒ…å ±ã®é€†è³ªå•
    # -------------------------------------------------
    elif mode == "deep":
        # æ—¢ã«ãƒ’ã‚¢ãƒªãƒ³ã‚°æ¸ˆã¿ã®æƒ…å ±ï¼ˆcontextï¼‰ãŒã‚ã‚‹å ´åˆ
        if context:
            system_prompt = """
ã‚ãªãŸã¯ã€Œè¦ä»¶å®šç¾©ã®ãƒ—ãƒ­ã€ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã€Œæœ€åˆã®è¦æœ›ã€ã¨ã€è³ªå•ã«å¯¾ã™ã‚‹ã€Œè¿½åŠ å›ç­”ã€ã‚’çµ±åˆã—ã€
æœ€çµ‚çš„ãªå®Œæˆç‰ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

å‡ºåŠ›ã¯å®Œæˆç‰ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã¿ã€‚
"""
            user_prompt = f"ã€å½“åˆã®è¦æœ›ã€‘\n{text}\n\nã€è¿½åŠ å›ç­”ã€‘\n{context}\n\nã€å®Œæˆç‰ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‘"
            
            try:
                response = current_model.generate_content(system_prompt + user_prompt)
                return {"type": "complete", "text": response.text.strip()}
            except Exception as e:
                return {"type": "error", "text": str(e)}
        
        # åˆå›ï¼šè¶³ã‚Šãªã„æƒ…å ±ã‚’æ¢ã‚‹
        else:
            system_prompt = """
ã‚ãªãŸã¯ã€Œæ…é‡ãªã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã€ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æœ›ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«ã€Œè‡´å‘½çš„ã«è¶³ã‚Šãªã„æƒ…å ±ã€ã‚„ã€Œç¢ºèªã™ã¹ãæ›–æ˜§ãªç‚¹ã€ãŒã‚ã‚Œã°ã€
ãã‚Œã‚’ãŸãšã­ã‚‹è³ªå•ã‚’1ã¤ã€œ2ã¤ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã‚‚ã—æƒ…å ±ãŒååˆ†ã§ã‚ã‚Œã°ã€ãã®ã¾ã¾æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

å‡ºåŠ›å½¢å¼ã¯å³å¯†ã«JSONã§:
{
    "status": "question" ã¾ãŸã¯ "complete",
    "content": "è³ªå•æ–‡" ã¾ãŸã¯ "å®Œæˆã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"
}
"""
            user_prompt = f"ã€è¦æœ›ã€‘\n{text}"
            
            try:
                response = current_model.generate_content(
                    system_prompt + user_prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.2,
                        response_mime_type="application/json"
                    )
                )
                
                result = json.loads(response.text)
                
                if result.get("status") == "question":
                    return {"type": "question", "text": result["content"]}
                else:
                    return {"type": "complete", "text": result["content"]}
                    
            except json.JSONDecodeError:
                # JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã¯ãã®ã¾ã¾è¿”ã™
                return {"type": "complete", "text": response.text.strip()}
            except Exception as e:
                return {"type": "error", "text": str(e)}

    return {"type": "error", "text": f"Unknown mode: {mode}"}


@app.post("/filter", response_model=FilterResponse)
async def filter_prompt(body: FilterRequest):
    """
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸‹å‡¦ç†ã™ã‚‹ï¼ˆ3ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰
    
    - **raw/light**: æ¹¯ãŒã - ãƒã‚¤ã‚ºé™¤å»ã®ã¿ã€é«˜é€Ÿ
    - **heavy**: ç‚™ã‚‹ - æ§‹é€ åŒ–ã€6W3Hè£œå®Œ
    - **deep**: æ·±æ˜ã‚Š - é€†è³ªå•ã§ä¸è¶³æƒ…å ±ã‚’å–å¾—
    """
    print(f"ğŸ¥¦ å‡¦ç†é–‹å§‹ ({body.mode}): {body.text[:20]}...")
    
    current_model = get_model()
    if current_model is None:
        raise HTTPException(
            status_code=503,
            detail="Gemini APIãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚GEMINI_API_KEYç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        )
    
    result = process_prompt(body.text, body.mode, body.context, current_model)
    
    print(f"âœ¨ çµæœ ({result['type']}): \n{result['text'][:50]}...")
    return FilterResponse(type=result["type"], text=result["text"], mode=body.mode)


# --- èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆ ---
if __name__ == "__main__":
    import uvicorn
    
    print("ğŸš€ AI Clipboard API Server")
    print("-" * 40)
    print("ğŸ“– API ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: http://localhost:8000/docs")
    print("ğŸ”§ ç’°å¢ƒå¤‰æ•° GEMINI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    print("-" * 40)
    
    uvicorn.run(app, host="0.0.0.0", port=8000)

