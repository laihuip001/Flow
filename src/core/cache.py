import logging
import hashlib
from typing import Optional
from sqlalchemy.orm import Session
from datetime import datetime
import asyncio

from .models import PrefetchCache
from .types import ProcessingSuccess
from .seasoning import SeasoningManager, RESOLVED_LIGHT, RESOLVED_MEDIUM, RESOLVED_RICH

logger = logging.getLogger("core_cache")

class CacheManager:
    """
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ãƒ»Prefetchãƒ­ã‚¸ãƒƒã‚¯ã®è²¬å‹™ã‚’æŒã¤ã‚¯ãƒ©ã‚¹ (v5.0 Phase 1)
    """

    @staticmethod
    def get_text_hash(text: str) -> str:
        return hashlib.sha256(text.encode()).hexdigest()[:32]

    @staticmethod
    def sanitize_log(text: str) -> str:
        """ãƒ­ã‚°ç”¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚ºï¼ˆãƒãƒƒã‚·ãƒ¥åŒ–ï¼‰"""
        if not text:
            return "[empty]"
        text_hash = CacheManager.get_text_hash(text)[:8]
        return f"[text:{text_hash}...len={len(text)}]"

    def _check_ttl(self, cache: PrefetchCache) -> bool:
        """
        TTL (Time To Live / è³žå‘³æœŸé™) ãƒã‚§ãƒƒã‚¯
        æœŸé™åˆ‡ã‚Œãªã‚‰ True ã‚’è¿”ã™
        """
        from .config import settings
        from datetime import timedelta
        
        if not cache.created_at:
            return False

        limit = settings.CACHE_TTL_HOURS
        deadline = cache.created_at + timedelta(hours=limit)
        
        # ç¾åœ¨æ™‚åˆ»ãŒæœŸé™ã‚’éŽãŽã¦ã„ãŸã‚‰ True (Expired)
        if datetime.utcnow() > deadline:
            logger.info(f"ðŸ—‘ï¸ Cache Expired: {cache.hash_id[:8]} (Created: {cache.created_at})")
            return True
        return False

    def _enforce_limit(self, db: Session):
        """
        LRU (Least Recently Used / å®¹é‡åˆ¶é™) ãƒã‚§ãƒƒã‚¯
        ä¸Šé™ã‚’è¶…ãˆã¦ã„ãŸã‚‰ã€ä¸€ç•ªå¤ã„ã‚¢ã‚¯ã‚»ã‚¹ã®ã‚‚ã®ã‚’å‰Šé™¤
        """
        from .config import settings
        
        max_entries = settings.CACHE_MAX_ENTRIES
        count = db.query(PrefetchCache).count()
        
        if count > max_entries:
            # æº¢ã‚ŒãŸåˆ†ã ã‘å‰Šé™¤ï¼ˆå¿µã®ç‚ºãƒ«ãƒ¼ãƒ—ã§ã¯ãªãä¸€æ‹¬å‰Šé™¤ã‚’æ¤œè¨Žã™ã‚‹ãŒã€ã“ã“ã§ã¯1ä»¶ãšã¤å¤ã„é †ï¼‰
            over = count - max_entries
            logger.info(f"ðŸ§¹ Cache Limit Exceeded ({count} > {max_entries}). Cleaning {over} items...")
            
            # last_accessed_at ãŒå¤ã„é †ã«å–å¾—ã—ã¦ä¸€æ‹¬å‰Šé™¤
            # SQLite does not support DELETE ... LIMIT directly in standard SQLAlchemy ORM bulk delete easily without subquery
            # but usually, fetching IDs and deleting by ID is safer and widely supported.
            
            victims = db.query(PrefetchCache.hash_id).order_by(PrefetchCache.last_accessed_at.asc()).limit(over).all()
            victim_ids = [v[0] for v in victims]
            
            if victim_ids:
                db.query(PrefetchCache).filter(PrefetchCache.hash_id.in_(victim_ids)).delete(synchronize_session=False)
                db.commit()

    def check_cache(self, db: Session, text: str, seasoning: int) -> Optional[ProcessingSuccess]:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¤œç´¢ã—ã€ãƒ’ãƒƒãƒˆã™ã‚Œã°çµæžœã‚’è¿”ã™ã€‚
        ãƒ’ãƒƒãƒˆã—ãªã„å ´åˆã¯Noneã‚’è¿”ã™ã€‚
        """
        if db is None:
            return None

        text_hash = self.get_text_hash(text)
        cache_key = f"seasoning_{seasoning}"

        try:
            cache = db.query(PrefetchCache).filter(PrefetchCache.hash_id == text_hash).first()
            
            # 1. TTL Check
            if cache and self._check_ttl(cache):
                # Expired: Treat as miss (Cleanup happens later or explicitly now?)
                # ç°¡æ˜“çš„ã«ã“ã“ã§å‰Šé™¤ or ç„¡è¦–ã€‚ã“ã“ã§ã¯å‰Šé™¤ã—ã¦ã—ã¾ã†ã®ãŒã‚¯ãƒªãƒ¼ãƒ³ã€‚
                db.delete(cache)
                db.commit()
                return None

            if cache and cache.results and cache_key in cache.results:
                cached_result = cache.results[cache_key]
                
                # ã‚¨ãƒ©ãƒ¼æ–‡å­—åˆ—ãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãƒ’ãƒƒãƒˆæ‰±ã„ã—ãªã„ï¼ˆå†è©¦è¡Œã•ã›ã‚‹ï¼‰
                if cached_result.startswith("Error:"):
                    return None

                # 2. LRU Update
                cache.last_accessed_at = datetime.utcnow()
                db.commit()

                logger.info(f"ðŸ“¦ Cache Hit: {CacheManager.sanitize_log(cached_result)}")
                return {
                    "result": cached_result,
                    "seasoning": seasoning,
                    "from_cache": True,
                    "model_used": None
                }
        except Exception as e:
            logger.warning(f"âš ï¸ Cache check failed: {e}")
            return None
        
        return None

    # --- v5.0 Phase 3: Warmup Logic ---
    async def warmup_from_list(self, db: Session, templates: list[str], client, privacy, callback=None, force: bool = False) -> dict:
        """
        ãƒªã‚¹ãƒˆå†…ã®å®šåž‹æ–‡ã«ã¤ã„ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆã™ã‚‹ï¼ˆWarmupï¼‰ã€‚
        ç›´åˆ—å®Ÿè¡Œï¼‹Waitã«ã‚ˆã‚Šãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆã‚’å›žé¿ã™ã‚‹ã€‚

        Args:
            db: Database session
            templates: æ–‡å­—åˆ—ãƒªã‚¹ãƒˆ
            client: GeminiClient instance
            privacy: PrivacyHandler instance
            callback: fn(current, total, text) -> None
            force: Trueãªã‚‰æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å†ç”Ÿæˆ

        Returns:
            dict: å‡¦ç†çµæžœçµ±è¨ˆ
        """
        stats = {"total": len(templates), "processed": 0, "skipped": 0, "errors": 0}
        levels = [RESOLVED_LIGHT, RESOLVED_MEDIUM, RESOLVED_RICH]
        batch_size = 5  # M-03: Batch commit interval
        pending_commits = 0

        for i, text in enumerate(templates):
            text = text.strip()
            if not text:
                continue

            if callback:
                callback(i + 1, len(templates), text)

            try:
                text_hash = self.get_text_hash(text)
                cache = db.query(PrefetchCache).filter(PrefetchCache.hash_id == text_hash).first()
                
                current_results = {}
                
                if not cache:
                    # New Item
                    cache = PrefetchCache(hash_id=text_hash, original_text=text, results={})
                    # Don't add yet, merge later
                else:
                    if not force and cache.results and len(cache.results) >= 3:
                        stats["skipped"] += 1
                        logger.debug(f"Skipped: {text[:10]}...")
                        if callback: callback(i + 1, len(templates), f"{text[:20]} (Skip)")
                        continue
                    current_results = dict(cache.results) if cache.results else {}

                item_updated = False
                
                for season in levels:
                    key = f"seasoning_{season}"
                    if key in current_results and not force:
                        continue

                    # --- Generate ---
                    if callback: callback(i + 1, len(templates), f"{text[:20]} ({season})")
                    
                    masked, mapping = privacy.mask(text)
                    system_prompt = SeasoningManager.get_system_prompt(season)
                    
                    config = {
                        "system": system_prompt,
                        "params": {"temperature": 0.3}
                    }

                    # Call API
                    res = await client.generate_content(masked, config, model=None)

                    if res["success"]:
                        final_text = res["result"]
                        if mapping:
                            final_text = privacy.unmask(final_text, mapping)
                        
                        current_results[key] = final_text
                        item_updated = True
                        
                        # H-03: Exponential backoff for rate limiting
                        base_delay = 1.5
                        await asyncio.sleep(base_delay)
                    else:
                        err_msg = res.get('error')
                        reason = res.get('blocked_reason')
                        full_msg = f"{err_msg} ({reason})" if reason else err_msg
                        logger.error(f"âŒ API Error for '{text[:10]}' ({season}): {full_msg}")
                        stats["errors"] += 1

                if item_updated:
                    cache.results = current_results
                    cache.updated_at = datetime.utcnow() # T-1: Valid now (Column added)
                    db.merge(cache)
                    pending_commits += 1
                    stats["processed"] += 1
                    
                    # M-03: Batch commits
                    if pending_commits >= batch_size:
                        db.commit()
                        pending_commits = 0
                        # Enforce Limit after batch commit
                        self._enforce_limit(db)
                        
                elif not cache.results: # Is new but failed all?
                     # If new and no results, commit emptiness or skip logic handled above
                     pass

            except Exception as e:
                import traceback
                traceback.print_exc()
                logger.error(f"Example {CacheManager.sanitize_log(text)} failed: {e}")
                stats["errors"] += 1
                db.rollback()

        # Final commit and enforcement
        if pending_commits > 0:
            db.commit()
            self._enforce_limit(db)

        return stats
